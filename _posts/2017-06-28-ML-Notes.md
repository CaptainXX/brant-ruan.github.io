---
title: Coursera | Machine Learning Notes
category: CS
---

## Coursera | Machine Learning Notes

### Preface

**All the resources here belong to Andrew Ng and relative staffs.**

**I download them from [Coursera](https://www.coursera.org/).**

**Sincerely, thanks for your teaching and your sharing of knowledge and technique.**

**Thank you, Andrew Ng.**

### Slides

- [Lecture1.pdf]({{ site.url }}/resources/pdf/Lecture1.pdf)
- [Lecture2.pdf]({{ site.url }}/resources/pdf/Lecture2.pdf)
- [Lecture3.pdf]({{ site.url }}/resources/pdf/Lecture3.pdf)
- [Lecture4.pdf]({{ site.url }}/resources/pdf/Lecture4.pdf)
- [Lecture5.pdf]({{ site.url }}/resources/pdf/Lecture5.pdf)
- [Lecture6.pdf]({{ site.url }}/resources/pdf/Lecture6.pdf)
- [Lecture7.pdf]({{ site.url }}/resources/pdf/Lecture7.pdf)
- [Lecture8.pdf]({{ site.url }}/resources/pdf/Lecture8.pdf)
- [Lecture9.pdf]({{ site.url }}/resources/pdf/Lecture9.pdf)

### Videos

**Week 1**

- [Welcome to Machine Learning]({{ site.url }}/resources/videos/ml-week1-1-welcome-to-machine-learning.mp4)

**Week 2**

**Week 3**

**Week 4**

**Week 5**

**Week 6**

**Week 7**

**Week 8**

**Week 9**

**Week 10**

**Week 11**

### Programming Tasks

- [machine-learning-ex1.zip]({{ site.url }}/resources/code/machine-learning-ex1.zip)
- [machine-learning-ex2.zip]({{ site.url }}/resources/code/machine-learning-ex2.zip)
- [machine-learning-ex3.zip]({{ site.url }}/resources/code/machine-learning-ex3.zip)
- [machine-learning-ex4.zip]({{ site.url }}/resources/code/machine-learning-ex4.zip)


### My Notes

#### Week 1

#### Week 2

#### Week 3

#### Week 4

#### Week 5

##### BPNN Implement Notes

When implementing BPNN, you should be clear about the exact definition of `z` and `a`, that is:

{% highlight matlab %}
delta2 = zeros(hidden_layer_size, 1);
delta3 = zeros(num_labels, 1);
Delta2 = zeros(hidden_layer_size, input_layer_size + 1);
Delta3 = zeros(num_labels, hidden_layer_size + 1);

for t = 1:m,
% Note that we have put bias unit in X above
	a1 = transpose(X(t, :));
	% Feedforward
	z2 = Theta1 * a1;
	a2 = [1; sigmoid(z2)];
	z3 = Theta2 * a2;
	a3 = sigmoid(z3);
% backpropagation
	temp_y = zeros(num_labels, 1);
	temp_y(y(t)) = 1;
	delta3 = a3 - temp_y;
	delta2 = (transpose(Theta2) * delta3)(2:end) .* sigmoidGradient(z2);
	Delta3 = Delta3 + delta3 * transpose(a2);
	Delta2 = Delta2 + delta2 * transpose(a1);
end

Theta1_grad = Delta2 ./ m;
Theta2_grad = Delta3 ./ m;
{% endhighlight %}

Be clear that there exists only bias for `a` and no bias for `z`. And layer `l` doesn't contribute to the `bias` in layer `i+1`. So note that

```
z2 = Theta1 * a1;
z2 = [1; z2];
a2 = sigmoid(z2);
```

is **WRONG**. You should calculate as

```
z2 = Theta1 * a1;
a2 = [1; sigmoid(z2)];
```

Finally, pay attention to the size of `delta` and `Delta`.

#### Week 6

#### Week 7

#### Week 8

#### Week 9

#### Week 10

#### Week 11

### Summary



### Reference

- [Coursera: Machine Learning](https://www.coursera.org/learn/machine-learning/)